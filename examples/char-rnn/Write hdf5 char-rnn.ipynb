{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class exporthdf5:\n",
    "    def __init__(self, encoding, input_txt, vocab_filename):\n",
    "        self.encoding = encoding\n",
    "        self.input_txt = input_txt\n",
    "        #encoding = 'utf-8'\n",
    "        #input_txt = 'data/tiny-shakespeare.txt'\n",
    "        #vocab_filename = 'vocabulary.txt'\n",
    "        val_frac = 0.1\n",
    "        test_frac = 0.1\n",
    "\n",
    "\n",
    "        # First go the file once to see how big it is and to build the vocab\n",
    "        self.token_to_idx = {}\n",
    "        self.total_size = 0\n",
    "        with codecs.open(input_txt, 'r', encoding) as f:\n",
    "            for line in f:\n",
    "              self.total_size += len(line)\n",
    "              for char in line:\n",
    "                if char not in self.token_to_idx:\n",
    "                  self.token_to_idx[char] = len(self.token_to_idx) + 1\n",
    "\n",
    "\n",
    "        #Write vocab to file\n",
    "        arr = [self.token_to_idx.keys()[self.token_to_idx.values().index(index)] for index in xrange(1, len(self.token_to_idx) + 1)]\n",
    "\n",
    "        print 'Dumping vocabulary to file: %s' % vocab_filename\n",
    "        with open(vocab_filename, 'wb') as vocab_file:\n",
    "          for char in arr:\n",
    "            vocab_file.write('%s\\n' % char.encode('unicode_escape'))\n",
    "        print 'Done.'\n",
    "\n",
    "         # Now we can figure out the split sizes\n",
    "        self.val_size = int(val_frac * self.total_size)\n",
    "        self.test_size = int(test_frac * self.total_size)\n",
    "        self.train_size = self.total_size - self.val_size - self.test_size\n",
    "\n",
    "        print 'Total vocabulary size: %d' % len(self.token_to_idx)\n",
    "        print 'Total tokens in file: %d' % self.total_size\n",
    "        print '  Training size: %d' % self.train_size\n",
    "        print '  Val size: %d' % self.val_size\n",
    "        print '  Test size: %d' % self.test_size\n",
    "        \n",
    "    def create_dataset(self, name, start_offset, size):\n",
    "        MAX_CHARS = 50\n",
    "        BUFFER_SIZE = 100\n",
    "\n",
    "        num_samples = math.ceil(size / float(MAX_CHARS))\n",
    "        print 'Number of samples: %f' % num_samples\n",
    "        remainder = num_samples % BUFFER_SIZE\n",
    "        num_needed = BUFFER_SIZE - remainder\n",
    "        print 'Samples needed: %f' % num_needed\n",
    "\n",
    "        samples = []\n",
    "        targets = []\n",
    "\n",
    "        with codecs.open(self.input_txt, 'r', self.encoding) as f:\n",
    "            # Read the file\n",
    "            contents = f.read()\n",
    "            contents_t = contents[start_offset+1:start_offset+size+1]\n",
    "            contents = contents[start_offset:start_offset+size]\n",
    "            # while the contents contain something\n",
    "            while contents:\n",
    "                # Add the first 50 characters to the grouping\n",
    "                samples.append(contents[:50])\n",
    "                # Set the contents to everything after the first 50\n",
    "                contents = contents[50:]\n",
    "\n",
    "            # while the contents contain something\n",
    "            while contents_t:\n",
    "                # Add the first 50 characters to the grouping\n",
    "                targets.append(contents_t[:50])\n",
    "                # Set the contents to everything after the first 50\n",
    "                contents_t = contents_t[50:]\n",
    "\n",
    "        # fill up randomly\n",
    "        for i in range(int(num_needed)):\n",
    "            choice = random.randint(0, num_samples - 1)\n",
    "            samples.append(samples[choice])\n",
    "            targets.append(targets[choice])\n",
    "\n",
    "        samples_per_stream = len(samples) / BUFFER_SIZE\n",
    "        stream_length = samples_per_stream * MAX_CHARS\n",
    "        print 'Samples per stream: %d, Single stream length: %d' % (samples_per_stream , stream_length)\n",
    "\n",
    "        train = np.zeros((stream_length, BUFFER_SIZE), dtype=np.float32)\n",
    "        target = np.zeros((stream_length, BUFFER_SIZE), dtype=np.float32)\n",
    "        cont = np.zeros((stream_length, BUFFER_SIZE), dtype=np.float32)\n",
    "\n",
    "        for stream_num in xrange(BUFFER_SIZE):\n",
    "            for sample_num in xrange(samples_per_stream):\n",
    "                # for train\n",
    "                sample = samples[stream_num*samples_per_stream+sample_num]\n",
    "                sample_chars = [self.token_to_idx[index] for index in sample]\n",
    "\n",
    "                # for target        \n",
    "                tgt = targets[stream_num*samples_per_stream+sample_num]\n",
    "                target_chars = [self.token_to_idx[index] for index in tgt]\n",
    "\n",
    "                # fill 1s every time we have a char that should be learned\n",
    "                cont_tmp = np.zeros(MAX_CHARS, dtype=np.uint8)\n",
    "                cont_tmp[:len(sample_chars)] = [1] * len(sample_chars)\n",
    "\n",
    "                if (len(sample_chars)<MAX_CHARS):\n",
    "                    # fill up to MAX_CHARS\n",
    "                    sample_chars[len(sample_chars):MAX_CHARS] = [0] * (MAX_CHARS-len(sample_chars))\n",
    "                    # fill 0s every time we have a char that should NOT be learned\n",
    "                    cont_tmp[len(sample_chars):MAX_CHARS]  = [0] * (MAX_CHARS-len(sample_chars))\n",
    "\n",
    "\n",
    "                # for target\n",
    "                if (len(target_chars)<MAX_CHARS):\n",
    "                    # fill up to MAX_CHARS\n",
    "                    target_chars[len(target_chars):MAX_CHARS] = [0] * (MAX_CHARS-len(target_chars))\n",
    "\n",
    "                train[sample_num*MAX_CHARS:(sample_num+1)*MAX_CHARS,stream_num] = sample_chars   \n",
    "                cont[sample_num*MAX_CHARS:(sample_num+1)*MAX_CHARS,stream_num] = cont_tmp\n",
    "                target[sample_num*MAX_CHARS:(sample_num+1)*MAX_CHARS,stream_num] = target_chars\n",
    "\n",
    "        with h5py.File(name, 'w') as f:\n",
    "            f.create_dataset('input', data = train)\n",
    "            f.create_dataset('cont', data = cont)\n",
    "            f.create_dataset('target', data = target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping vocabulary to file: vocab_shakespeare.txt\n",
      "Done.\n",
      "Total vocabulary size: 65\n",
      "Total tokens in file: 1115394\n",
      "  Training size: 892316\n",
      "  Val size: 111539\n",
      "  Test size: 111539\n"
     ]
    }
   ],
   "source": [
    "shakespeare = exporthdf5('utf-8', 'data/tiny-shakespeare.txt', 'vocab_shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 17847.000000\n",
      "Samples needed: 53.000000\n",
      "Samples per stream: 179, Single stream length: 8950\n"
     ]
    }
   ],
   "source": [
    "shakespeare.create_dataset('data/shakespeare_train.h5', 0, shakespeare.train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 2231.000000\n",
      "Samples needed: 69.000000\n",
      "Samples per stream: 23, Single stream length: 1150\n"
     ]
    }
   ],
   "source": [
    "shakespeare.create_dataset('data/shakespeare_val.h5', shakespeare.train_size, shakespeare.val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping vocabulary to file: vocab_reddit.txt\n",
      "Done.\n",
      "Total vocabulary size: 656\n",
      "Total tokens in file: 2688329\n",
      "  Training size: 2150665\n",
      "  Val size: 268832\n",
      "  Test size: 268832\n"
     ]
    }
   ],
   "source": [
    "reddit = exporthdf5('utf-8', 'data/reddit.txt', 'vocab_reddit.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 43014.000000\n",
      "Samples needed: 86.000000\n",
      "Samples per stream: 431, Single stream length: 21550\n"
     ]
    }
   ],
   "source": [
    "reddit.create_dataset('data/reddit_train.h5', 0, reddit.train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 5377.000000\n",
      "Samples needed: 23.000000\n",
      "Samples per stream: 54, Single stream length: 2700\n"
     ]
    }
   ],
   "source": [
    "reddit.create_dataset('data/reddit_val.h5', reddit.train_size, reddit.val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print example\n",
    "print train.shape\n",
    "\n",
    "tmp_arr = [token_to_idx.keys()[token_to_idx.values().index(index)] for index in train[:,0]]\n",
    "\n",
    "s = ''\n",
    "for char in tmp_arr:\n",
    "    s += char\n",
    "    \n",
    "print s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
